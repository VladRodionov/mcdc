
# ğŸ¥• MCDC â€” Memcached with Dictionary Compression

**MCDC** (Memcached Dictionary Compression) is a **drop-in replacement for Memcached 1.6.38** with built-in 
**Zstandard dictionary compression**, delivering 1.5Ã—â€“2.5Ã— better RAM efficiency without any client-side 
changes. MCDC is developed and maintained by [Carrot Data](https://github.com/carrotdata) â€” a project focused on practical, 
SSD-friendly, memory-efficient caching technologies.

---

## ğŸ§© What Is MCDC?

MCDC extends Memcached with adaptive, server-side data compression.  Instead of compressing 
each object individually (like client SDKs usually do), MCDC automatically learns shared byte 
patterns across your data and uses them  to build a **Zstandard dictionary**. Once trained, 
the dictionary allows ultra-compact compression of small-to-medium, structurally similar objects â€” 
tweets, JSON fragments, log entries, etc. â€” at minimal CPU cost.

All standard Memcached commands, clients, and SDKs continue to work unchanged.

---

## ğŸ§  Dictionary Compression â€” Why It Matters


Typical â€œclient-side compressionâ€ operates in isolation: every key/value pair is compressed 
independently, often wasting space on per-object headers, redundant Huffman tables, and 
repeated symbol statistics. That approach is fine for large blobs, but ineffective for workloads
with millions of small-to-medium, structurally similar objects.

Most modern compressors â€” including Zstandard â€” are based on the LZ77 family of algorithms.
They work by replacing repeated sequences in the input with back-references to earlier
occurrences within a sliding window. When each object is compressed separately, the window
is limited to that objectâ€™s own content, so the compressor can only exploit patterns that
repeat within the same value. **All cross-object redundancy is lost**.

Dictionary compression changes the game: instead of a small, local window, the compressor
has access to a shared, pre-trained dictionary containing common byte sequences found across
many objects. Each new value can now encode long back-references into this large shared base,
reusing the same token and Huffman tables for the entire workload. The result is far fewer
literal bytes and much higher compression ratios â€” especially for small JSON, text, or
message-like objects that share similar structure.

In short, a client-side compressor can only see inside a single message, while MCDCâ€™s server-side
dictionary compression sees across the entire dataset. That global view enables back-references 
to patterns every client shares â€” a capability fundamentally impossible when compressing each 
item in isolation.

**Dictionary compression** fixes that inefficiency:

1. **Training** â€” MCDC collects samples of your workload and trains a shared Zstandard dictionary.  
2. **Encoding** â€” New values are compressed using that dictionary; redundant structure vanishes.  
3. **Adapting** â€” The dictionary is periodically retrained as data evolves.

The result is smaller objects, lower memory usage, and less network traffic â€”  without touching 
your application code or client libraries.

â¸»

## ğŸ§  Facebook Managed Compression

The first adopter â€” and arguably the inventor â€” of this approach was the Facebook Engineering Team.
They introduced it under the name â€œManaged Compressionâ€ in a 2018 engineering blog post:

ğŸ‘‰ [5 Ways Facebook Improved Compression at Scale with Zstandard](https://engineering.fb.com/2018/12/19/core-infra/zstandard/)

> â€œOne important use case for managed compression is compressing values in caches.
> Traditional compression is not very effective in caches, since each value must be compressed individually.
> Managed compression exploits knowledge of the structure of the key (when available) to intelligently group values into self-similar categories.
> It then trains and ships dictionaries for the highest-volume categories.
> The gains vary by use case but can be significant.
> Our caches can store up to 40 percent more data using the same hardware.â€
â€” Facebook Engineering Blog, 2018

â¸»

## ğŸ” How MCDC Differs

While Facebookâ€™s *Managed Compression* was a pioneering concept, **MCDC** takes the idea further â€” making it fully self-contained and open-source.

| **Aspect** | **Facebook Managed Compression** | **MCDC (Memcached Dictionary Compression)** |
|-------------|----------------------------------|---------------------------------------------|
| Architecture | Centralized *Managed Compression Service* | Integrated entirely inside the caching server |
| Dictionary Training | Performed externally by a centralized service | Done automatically inside MCDC at runtime |
| Monitoring | Centralized telemetry and workload tracking | Local, adaptive workload analysis |
| Dictionary Lifecycle | Pushed periodically from a central service | Trained, validated, and rotated on the fly |
| Availability | Proprietary (internal to Facebook) | 100% open-source |
| Deployment Complexity | Requires coordination with an external system | Drop-in replacement for Memcached â€” no dependencies |


â¸»

ğŸ§© Why It Matters

By embedding training, monitoring, and dictionary management directly inside the cache, MCDC eliminates the need for any external coordination service â€” achieving
the benefits of Managed Compression with zero infrastructure overhead.

In short:

> Facebook invented it, MCDC democratized it.

â¸»

ğŸ“ˆ Real-World Impact

Real-world deployments have shown that dictionary-based compression dramatically improves cache efficiency â€” especially for workloads dominated by small and repetitive objects (e.g. JSONs, tweets, logs, or structured metadata).

ğŸš€ What We Know from Industry
- Facebook reported up to 40 % more data stored on the same hardware using managed compression.
- Their gains were consistent across multiple caching tiers and data types â€” demonstrating that even modest redundancy across keys can yield major savings once a shared dictionary is applied.

ğŸ§® What to Expect from MCDC

MCDC integrates this idea directly into the caching layer, so you can expect:
- 1.5 Ã— â€“ 2.5 Ã— memory savings on typical web-scale datasets
- Negligible CPU overhead thanks to the use of Zstandardâ€™s dictionary mode and adaptive training heuristics
- Automatic adaptation â€” as your workload evolves, MCDC retrains dictionaries and updates them without downtime
- Compression ratios that outperform any client-side compression, because MCDC compresses across similar objects, not just within each value

â¸»

## âœ¨ Key Features

- ğŸ”¹ **Drop-in replacement** â€” fully compatible with Memcached 1.6.x (its a fork of 1.6.38) 
- ğŸ”¹ **Zstandard dictionary compression** with automatic retraining  
- ğŸ”¹ **Dynamic sampling** and adaptive dictionary updates  
- ğŸ”¹ **Namespace-aware** compression and statistics  
- ğŸ”¹ **Low CPU overhead** â€” typical impact under 20-30 % at 2Ã— memory savings  
- ğŸ”¹ **JSON-based configuration and introspection commands**  
- ğŸ”¹ **Zero client changes, no proxy required**

---

## ğŸ§­ New Commands

| Command | Description |
|----------|-------------|
| `mcz config [json]` | Prints current MCDC configuration |
| `mcz stats [namespace] [json]` | Returns detailed compression and memory statistics global and per-namespace|
| `mcz ns` | Lists active namespaces and their dictionaries |
| `mcz reload [json]` | Forces dictionaries reload |
| `mcz sampler [start stop status]` | Controls data spooling (for offline training) |

All new commands follow the Memcached text protocol and can be tested via `telnet` or `nc`.

---

## ğŸ§± Prerequisites

You need standard Memcached build dependencies plus Zstandard.

Supported toolchains:
-	GCC 11+  / Clang 14+
-	libevent â‰¥ 2.1, libzstd â‰¥ 1.5

Tested on Linux (x86-64 / aarch64) and macOS (arm64 / x86-64)

â¸»

## âš™ï¸ Build

```
git clone https://github.com/VladRodionov/mcdc.git
cd mcdc
git checkout mcdc
```

Linux(Ubuntu):
```bash
sudo apt-get update
sudo apt-get install -y build-essential autotools-dev pkgconf autoconf automake libtool m4 autoconf-archive \
    libevent-dev libzstd-dev
./autogen.sh
./configure --with-zstd --with-libevent
make
```

MacOS - use `build-mac.sh`

Youâ€™ll get:
```
memcached         # optimized release build + dictionary compression
memcached-debug   # debug
```

Example startup with dictionary compression enabled:

```
./memcached -m 3000 -z mcz.conf -p 11211
```

Example mcz.conf:

```
enable_comp=true
enable_dict=true
dict_dir=../dict-data
dict_size=1M
comp_level=3
min_comp_size=32
max_comp_size=256K

enable_training=true
retraining_interval=2h
min_training_size=0
ewma_alpha=0.20
retrain_drop=0.12
train_mode=fast
dict_retain_max=8

enable_sampling=true
sample_p=0.02
sample_window_duration=100
spool_dir=./samples
spool_max_bytes=64MB
```
For deployment, monitoring, and systemd service integration, see the official Memcached installation guide.
MCDC follows the same conventions and CLI options.

â¸»

## ğŸ“Š Benchmarks (Coming Soon)

This section will include reproducible membench results comparing
vanilla Memcached 1.6.38 vs MCDC under multiple datasets
(tweets, JSON objects, and mixed workloads).

Stay tuned â€” benchmark graphs and metrics will be published here soon.

â¸»

## ğŸ˜‚ Funny and Curious Moments During Development

Developing MCDC wasnâ€™t just compression ratios and profiler traces â€” there were some fun surprises:
- The Ancient Perl Tests:
Memcached still relies on a large suite of Perl regression tests.
Running them in 2025 felt like archaeology â€” proof that some code truly never dies.
- The â€œRandom Dataâ€ Paradox:
While verifying compression behavior, I discovered that
even random-looking sequences can compress efficientlyâ€¦ if the alphabet is small (e.g., Aâ€“Z).
Fewer than 256 unique symbols = exploitable structure for Zstandardâ€™s entropy coder.
- Wrong assumptions
Perl scripts assumed  that data is immutable, MCDCâ€™s dictionary blew those expectations apart â€” 
even Perl was surprised!

These moments reminded me that data â€œrandomnessâ€ is often an illusion â€” and that good compression 
still finds patterns in places humans donâ€™t.

â¸»

## ğŸ“œ License

MCDC is released under the Apache 2.0 License.
It includes and extends Memcached 1.6.38, which is distributed under a BSD-style license.
All trademarks and copyrights remain with their respective owners.

â¸»

## ğŸ™Œ Acknowledgements

MCDC builds upon two decades of exceptional work by the Memcached open-source community.
Special thanks to the maintainers for keeping Memcached simple, fast, and stable â€”
making it a perfect foundation for further innovation.

Developed with â¤ï¸ by Vlad Rodionov vladrodionov@gmail.com

My other projects: [Carrot Data](https://www.github.com/carrotdata)

â€œCache Smart, Save More.â€

---

